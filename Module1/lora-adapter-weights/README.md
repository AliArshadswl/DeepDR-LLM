If you have pretrained adapters by LoRA, place them in DeepDR-LLM/Module1/lora-adapter-weights
When training the DeepDR-LLM, just keep this folder empty